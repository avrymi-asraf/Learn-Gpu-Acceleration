# Learn using GPU acceleration

## Overview

This project aims to demonstrate the use of libraries that support GPU acceleration and illustrate the performance improvements they offer. The project uses Jupyter Notebooks for learning purposes.

## Goals

- Learn to use all libraries with GPU acceleration for various tasks, including data loading, processing, neural network training, and deploying models.
- Analyze runtimes for each library to determine the most efficient one in terms of GPU utilization.

## Libraries&#x20;

The following machine learning libraries are included:

- TensorFlow
- PyTorch
- scikit-learn (limited GPU support)
- XGBoost
- LightGBM
- RAPIDS (cuML)
- ONNX Runtime
- Transformers
- TensorRT
- TorchServe
- MLflow
- NVIDIA Triton Inference Server

## Getting Started

### Prerequisites

To run the benchmarks, you need:

- A computer with a GPU and up-to-date drivers (NVIDIA CUDA-capable GPU recommended).
- Python (>=3.8).
- CUDA Toolkit (version compatible with your GPU).
- Python package manager (`pip` or `conda`).

### Installation

Clone the repository and install dependencies:

Ensure that the CUDA Toolkit is correctly installed and available for GPU libraries to function properly.

## Using

Each library has an accompanying Jupyter Notebook that illustrates its usage, along with exercises for practice.

## Contributions

Contributions are welcome! If you have ideas for improving the benchmark process or want to add more libraries, feel free to create a pull request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

Thanks to the open-source community for their incredible machine learning tools and libraries, which make this kind of research possible.

## Contact

For any questions, please open an issue on GitHub or reach out to [your.email@example.com](mailto\:your.email@example.com).

